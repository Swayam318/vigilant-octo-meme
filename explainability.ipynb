{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18fe43-6747-42a7-a782-a429d6ab7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = '/kaggle/working/new_model.pth' # The CNN trained in Step 2\n",
    "CSV_PATH = '/kaggle/working/processed_train_new.csv' # We visualize Train data to compare with known prices\n",
    "NUM_SAMPLES = 10 # Number of images to generate\n",
    "\n",
    "# --- 1. Re-define Model Architecture (Must Match Training) ---\n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self, num_tabular_features):\n",
    "        super(MultimodalNet, self).__init__()\n",
    "        \n",
    "        # Image Branch\n",
    "        try:\n",
    "            from torchvision.models import ResNet18_Weights\n",
    "            self.cnn = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        except:\n",
    "            self.cnn = models.resnet18(pretrained=True)\n",
    "            \n",
    "        # Match the projection layer from training\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, 64)\n",
    "        \n",
    "        # Tabular Branch (Structure must match for loading weights, even if unused for CAM)\n",
    "        self.tabular_branch = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "        \n",
    "    def forward(self, image, tabular):\n",
    "        x_img = self.cnn(image)\n",
    "        x_tab = self.tabular_branch(tabular)\n",
    "        x_combined = torch.cat((x_img, x_tab), dim=1)\n",
    "        output = self.fusion(x_combined)\n",
    "        return output\n",
    "\n",
    "# --- 2. Dataset Loader ---\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Exclude non-feature columns\n",
    "        exclude = ['id', 'date', 'price', 'log_price', 'image_path', 'date_int']\n",
    "        # Identify interaction features if they exist, otherwise stick to basics\n",
    "        # We reload the original processed columns\n",
    "        self.feature_cols = [c for c in self.data.columns if c not in exclude]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = str(row['image_path'])\n",
    "        \n",
    "        if img_path != 'nan' and img_path != 'None' and os.path.exists(img_path):\n",
    "            original_img = Image.open(img_path).convert('RGB')\n",
    "            original_img = original_img.resize((224, 224))\n",
    "        else:\n",
    "            original_img = Image.new('RGB', (224, 224), color=(0,0,0))\n",
    "            \n",
    "        image_tensor = self.transform(original_img)\n",
    "        tabular = torch.tensor(row[self.feature_cols].values.astype(np.float32))\n",
    "        \n",
    "        return image_tensor, tabular, np.array(original_img), row['id'], row.get('price', 0)\n",
    "\n",
    "# --- 3. Grad-CAM Implementation ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "        \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "        \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "        \n",
    "    def __call__(self, image_tensor, tabular_tensor):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(image_tensor.unsqueeze(0), tabular_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        output.backward()\n",
    "        \n",
    "        # Generate Heatmap\n",
    "        gradients = self.gradients.cpu().data.numpy()[0]\n",
    "        activations = self.activations.cpu().data.numpy()[0]\n",
    "        \n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        \n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "            \n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / (np.max(cam) + 1e-8)\n",
    "        \n",
    "        return cam, output.item()\n",
    "\n",
    "# --- 4. Main Execution ---\n",
    "def run_explainability():\n",
    "    print(\"Initializing Grad-CAM Visualization...\")\n",
    "    \n",
    "    dataset = InferenceDataset(CSV_PATH)\n",
    "    # Pick random indices\n",
    "    indices = np.random.choice(len(dataset), NUM_SAMPLES, replace=False)\n",
    "    \n",
    "    # Initialize Model\n",
    "    num_features = len(dataset.feature_cols)\n",
    "    model = MultimodalNet(num_features).to(DEVICE)\n",
    "    \n",
    "    # Load weights loosely (allowing for some missing keys if strict match fails)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    except RuntimeError:\n",
    "        print(\"Warning: Strict loading failed, trying non-strict...\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE), strict=False)\n",
    "    \n",
    "    # Target Layer: Last Convolutional Block of ResNet18\n",
    "    target_layer = model.cnn.layer4[-1]\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    print(f\"Generating explanations for {NUM_SAMPLES} samples...\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * ((NUM_SAMPLES + 2) // 3))) # Dynamic height\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img_tensor, tab_tensor, original_img_np, house_id, true_price = dataset[idx]\n",
    "        img_tensor = img_tensor.to(DEVICE)\n",
    "        tab_tensor = tab_tensor.to(DEVICE)\n",
    "        \n",
    "        # Generate Heatmap\n",
    "        heatmap, pred_log_price = grad_cam(img_tensor, tab_tensor)\n",
    "        \n",
    "        # Overlay\n",
    "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        superimposed = np.uint8(0.6 * original_img_np + 0.4 * heatmap_colored)\n",
    "        \n",
    "        # Plotting\n",
    "        ax = plt.subplot(NUM_SAMPLES // 3 + 1, 3, i + 1)\n",
    "        plt.imshow(superimposed)\n",
    "        plt.title(f\"ID: {house_id}\\nPrice: ${true_price:,.0f}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('explainability_report.png')\n",
    "    print(\"Done. Visualization saved to 'explainability_report.png'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_explainability()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
