{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d38c0f-cdcb-4fbd-a4fd-d951d3c0b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " BASELINE MODEL CHECK (Tabular Only)\n",
      "========================================\n",
      " RMSE     : $133,392.89\n",
      " R² Score : 0.8582\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# BASELINE MODEL (Tabular Only)\n",
    "# ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def check_baseline():\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\" BASELINE MODEL CHECK (Tabular Only)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    \n",
    "    # 2. Simple Preprocessing\n",
    "    # Convert date to number\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    train_df['date_int'] = train_df['date'].astype(np.int64) // 10**9\n",
    "    \n",
    "    # Define X (Features) and y (Target)\n",
    "    drop_cols = ['id', 'date', 'price']\n",
    "    X = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
    "    y = np.log1p(train_df['price']) # Log-transform target for better accuracy\n",
    "    \n",
    "    # 3. Split Data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 4. Train Random Forest (Standard Baseline)\n",
    "    rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # 5. Evaluate\n",
    "    val_preds_log = rf.predict(X_val)\n",
    "    val_preds = np.expm1(val_preds_log) # Reverse log transform\n",
    "    y_val_actual = np.expm1(y_val)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val_actual, val_preds))\n",
    "    r2 = r2_score(y_val_actual, val_preds)\n",
    "    \n",
    "    print(f\" RMSE     : ${rmse:,.2f}\")\n",
    "    print(f\" R² Score : {r2:.4f}\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# Run the check\n",
    "if __name__ == \"__main__\":\n",
    "    check_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f5164-3633-45c3-848d-45b463555dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------\n",
    "# HYBRID MODEL (Tabular + Visual)\n",
    "#-----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, VotingRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = '/kaggle/working/new_model.pth'\n",
    "\n",
    "# --- 1. Define Dataset & Model for Extraction (Same as before) ---\n",
    "class RealEstateDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = str(row['image_path'])\n",
    "        if img_path != 'nan' and img_path != 'None' and os.path.exists(img_path):\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except:\n",
    "                image = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "        else:\n",
    "            image = Image.new('RGB', (224, 224), color=(0, 0, 0))   \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, row['id']\n",
    "\n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalNet, self).__init__()\n",
    "        try:\n",
    "            from torchvision.models import ResNet18_Weights\n",
    "            self.cnn = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        except:\n",
    "            self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, 64) \n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.cnn(image)\n",
    "\n",
    "def extract_embeddings(csv_file, model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = RealEstateDataset(csv_file, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Extracting features from {csv_file}...\")\n",
    "    all_ids = []\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for images, ids in tqdm(loader):\n",
    "            images = images.to(DEVICE)\n",
    "            embeddings = model(images)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            all_ids.extend(ids.numpy())\n",
    "            \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    cols = ['id'] + [f'visual_{i}' for i in range(all_embeddings.shape[1])]\n",
    "    data = np.column_stack((all_ids, all_embeddings))\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    df['id'] = df['id'].astype(int)\n",
    "    return df\n",
    "\n",
    "# --- 2. Advanced Feature Engineering ---\n",
    "def add_spatial_clusters(train_df, test_df, n_clusters=20):\n",
    "    print(\"Adding Geospatial Clusters...\")\n",
    "    # Combine coords to learn global clusters\n",
    "    coords = pd.concat([train_df[['lat', 'long']], test_df[['lat', 'long']]])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(coords)\n",
    "    \n",
    "    train_df['cluster'] = kmeans.predict(train_df[['lat', 'long']])\n",
    "    test_df['cluster'] = kmeans.predict(test_df[['lat', 'long']])\n",
    "    \n",
    "    # One-hot encode the clusters? \n",
    "    # Tree models handle integers fine, but let's leave as int for now.\n",
    "    return train_df, test_df\n",
    "\n",
    "def add_interactions(df):\n",
    "    # Interaction: Big House * High Grade = Exponential Value\n",
    "    df['sqft_grade'] = df['sqft_living'] * df['grade']\n",
    "    # Interaction: Living Area / Lot Area = Density\n",
    "    df['density'] = df['sqft_living'] / (df['sqft_lot'] + 1)\n",
    "    return df\n",
    "\n",
    "# --- 3. Main Execution Flow ---\n",
    "def run_hybrid_pipeline():\n",
    "    # A. Feature Extraction\n",
    "    print(\"--- Step 1: Visual Features ---\")\n",
    "    model = MultimodalNet().to(DEVICE)\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    cnn_state_dict = {k.replace('cnn.', ''): v for k, v in state_dict.items() if 'cnn.' in k}\n",
    "    model.cnn.load_state_dict(cnn_state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    train_visual = extract_embeddings('/kaggle/working/processed_train_new.csv', model).drop_duplicates(subset=['id'])\n",
    "    test_visual = extract_embeddings('/kaggle/working/processed_test_new.csv', model).drop_duplicates(subset=['id'])\n",
    "\n",
    "    # B. Merge\n",
    "    print(\"--- Step 2: Merging & Engineering ---\")\n",
    "    train_df = pd.read_csv('/kaggle/input/new-dataset/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/new-dataset/test.csv')\n",
    "    \n",
    "    # Add Geospatial Clusters (NEW)\n",
    "    train_df, test_df = add_spatial_clusters(train_df, test_df)\n",
    "    \n",
    "    train_full = pd.merge(train_df, train_visual, on='id', how='left')\n",
    "    test_full = pd.merge(test_df, test_visual, on='id', how='left')\n",
    "\n",
    "    # C. Preprocess\n",
    "    def preprocess(df):\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['date_int'] = df['date'].astype(np.int64) // 10**9\n",
    "        \n",
    "        # Add Interaction Features (NEW)\n",
    "        df = add_interactions(df)\n",
    "        \n",
    "        drop_cols = ['id', 'date', 'price']\n",
    "        cols = [c for c in df.columns if c not in drop_cols]\n",
    "        df[cols] = df[cols].fillna(0)\n",
    "        return df[cols]\n",
    "\n",
    "    X = preprocess(train_full)\n",
    "    y = np.log1p(train_full['price'])\n",
    "    X_test = preprocess(test_full)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # D. Ensemble Training (NEW)\n",
    "    print(\"\\n--- Step 3: Training Ensemble (Voting Regressor) ---\")\n",
    "    \n",
    "    # Model 1: HistGradientBoosting (Fast & Accurate)\n",
    "    hgb = HistGradientBoostingRegressor(learning_rate=0.05, max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Model 2: XGBoost (The King of Tabular)\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=-1, random_state=42)\n",
    "    \n",
    "    # Model 3: Random Forest (Robustness)\n",
    "    rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "    \n",
    "    # Voting Regressor (Averages the predictions)\n",
    "    # We give slightly more weight to Gradient Boosting methods\n",
    "    ensemble = VotingRegressor(\n",
    "        estimators=[('hgb', hgb), ('xgb', xgb_model), ('rf', rf)],\n",
    "        weights=[2, 2, 1] \n",
    "    )\n",
    "    \n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_preds = np.expm1(ensemble.predict(X_val))\n",
    "    y_val_actual = np.expm1(y_val)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val_actual, val_preds))\n",
    "    r2 = r2_score(y_val_actual, val_preds)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\" FINAL ENSEMBLE RESULTS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\" RMSE     : ${rmse:,.2f}\")\n",
    "    print(f\" R² Score : {r2:.4f}\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "    \n",
    "    # E. Save Submission\n",
    "    test_preds = np.expm1(ensemble.predict(X_test))\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'predicted_price': test_preds\n",
    "    })\n",
    "    submission.to_csv('final_predictions.csv', index=False)\n",
    "    print(\"Ensemble Predictions saved to 'final_predictions.csv'.\")\n",
    "    \n",
    "    # F. Feature Importance (Using XGBoost for visualization)\n",
    "    print(\"Generating Feature Importance Plot...\")\n",
    "    xgb_model.fit(X_train, y_train) # Fit single model to get importance\n",
    "    xgb.plot_importance(xgb_model, max_num_features=15, height=0.5)\n",
    "    plt.title('Top 15 Features (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hybrid_feature_importance_new.png')\n",
    "    print(\"Plot saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_hybrid_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
